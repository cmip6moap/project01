{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating sensitivity ellipses for GlacierMIP following A.Grinsted's recipe\n",
    "\n",
    "#Questions: can we do this for GlacierMIP mean or do we need each model? Don't think we have this for SSPs...\n",
    "\n",
    "\n",
    "\n",
    "#Step zero:\n",
    "#Obtain time series of GMST and sea level contribution for each model run. \n",
    "\n",
    "\n",
    "#Source 1- Hock et al (2019)\n",
    "\n",
    "#RCP2.6  (46 model runs by 4 glacier models)\n",
    "#Global: 99.2+-14.3\n",
    "#Global exc Aa&GL; 70.5+-22.4\n",
    "\n",
    "#RCP4.5  (64 model runs by 4 glacier models)\n",
    "#Global: 141.9+-11.9\n",
    "#Global exc Aa&GL; 103.4+-16.0\n",
    "\n",
    "#RCP6.0  (16 model runs by 2 glacier models)\n",
    "#Global: 159.9+-0.0\n",
    "#Global exc Aa&GL; 105.4+-19.3\n",
    "\n",
    "#RCP8.5  (88 model runs by 6 glacier models)\n",
    "#Global: 198.9+-25.6\n",
    "#Global exc Aa&GL; 128.9+-36.0\n",
    "\n",
    "\n",
    "#Source 2- AR6 has SSPs from Marzeion and RCPs from Hock as well as AR5/SROCC. Both include Aa and GL periphery glaciers.\n",
    "#Sense check these against figures found in Hock- they use mean 2 rather than mean 1 (which was used in the paper!)\n",
    "\n",
    "#Source 3- Directly from Marzeion (2020)?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step one: \n",
    "#extract average rate and average temperature for every model run and for each of the target time periods. \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#Data into list of lists\n",
    "Hock_data = [['RCP2.6', 99.2, 2015, 2100,1], ['RCP4.6', 141.9,2015, 2100,2], ['RCP6.0', 159.9,2015, 2100,3], ['RCP8.5', 198.9,2015, 2100,4]]\n",
    "# Create pandas DataFrame\n",
    "df_Hock = pd.DataFrame(Hock_data, columns = ['Climate_Scenario', 'Global_SLR', 'Time_start', 'Time_end', 'Tavg']) \n",
    "calc_dSdt = df_Hock[\"Global_SLR\"]/(df_Hock[\"Time_end\"]-df_Hock[\"Time_start\"])\n",
    "df_Hock['dSdt'] = calc_dSdt\n",
    "\n",
    "\n",
    "#Data into list of lists\n",
    "HockAR6_data = [['RCP2.6', 94, 2015, 2100,'Tavg'], ['RCP4.6', 142, 2015, 2100,'Tavg'], ['RCP6.0', 200,2015, 2100,'Tavg'], ['RCP8.5', 198.9,2015, 2100,'Tavg']]\n",
    "# Create pandas DataFrame\n",
    "df_HockAR6 = pd.DataFrame(Hock_data, columns = ['Climate_Scenario', 'Global_SLR', 'Time_start', 'Time_end', 'Tavg']) \n",
    "calc_dSdt = df_HockAR6[\"Global_SLR\"]/(df_HockAR6[\"Time_end\"]-df_HockAR6[\"Time_start\"])\n",
    "df_HockAR6['dSdt'] = calc_dSdt\n",
    "\n",
    "\n",
    "\n",
    "#Add figures for T_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step two: extract slopes for each component \n",
    "#Given a target period like 2016-2050, we want to extract the sensitivity of every model we have. \n",
    "#So for every model, we want to select all the model runs, and fit a line. That should get us something like the yellow dots in the above plot. We should have multiple runs for each model as the model has been run for multiple scenarios.\n",
    "#We then want to fit a straight line (just using simple least squares). \n",
    "#The end result is we will have a table of slope, X-intercept, and the Y-intercept for each model and for each target period. We save that in a table with atleast the following columns: [Model_id, startyr, endyr, slope, Y-intercept, r2, ...] \n",
    "\n",
    "from scipy import stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df_Hock['Tavg'],df_HockAR6['dSdt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.831764705882353"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step Three: \n",
    "#Estimate location, size and orientation of the uncertainty ellipse. I.e. calculate the mean and uncertainty covariance matrix based on the results of step two. \n",
    "#Assumptions made: gaussian distribution; ensemble spread=uncertainty; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
